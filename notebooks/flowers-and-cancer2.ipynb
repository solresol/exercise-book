{
 "metadata": {
  "name": "",
  "signature": "sha256:2ba17a2f5be90ffaa757d5587088749b88720d901ec30797e06f7bb6b21a0985"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Logistic Regression Lab 2\n",
      "\n",
      "Scikit-Learn includes several sample datasets which can demonstrate\n",
      "logistic regression's usefulness.\n",
      "\n",
      "This is a very free-form lab: you won't be walked through it step-by-step,\n",
      "so you might want to keep some other examples open."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.datasets\n",
      "import pandas\n",
      "import sklearn.linear_model\n",
      "import sklearn.cross_validation\n",
      "import sklearn.metrics\n",
      "import numpy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will look at the Wisconsin breast cancer database, and a classic\n",
      "dataset of [different kinds of iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bc = sklearn.datasets.load_breast_cancer()\n",
      "print bc.DESCR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Breast Cancer Wisconsin (Diagnostic) Database\n",
        "\n",
        "Notes\n",
        "-----\n",
        "Data Set Characteristics:\n",
        "    :Number of Instances: 569\n",
        "\n",
        "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
        "\n",
        "    :Attribute Information:\n",
        "        - radius (mean of distances from center to points on the perimeter)\n",
        "        - texture (standard deviation of gray-scale values)\n",
        "        - perimeter\n",
        "        - area\n",
        "        - smoothness (local variation in radius lengths)\n",
        "        - compactness (perimeter^2 / area - 1.0)\n",
        "        - concavity (severity of concave portions of the contour)\n",
        "        - concave points (number of concave portions of the contour)\n",
        "        - symmetry \n",
        "        - fractal dimension (\"coastline approximation\" - 1)\n",
        "        \n",
        "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
        "        largest values) of these features were computed for each image,\n",
        "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
        "        13 is Radius SE, field 23 is Worst Radius.\n",
        "        \n",
        "        - class:\n",
        "                - WDBC-Malignant\n",
        "                - WDBC-Benign\n",
        "\n",
        "    :Summary Statistics:\n",
        "\n",
        "    ===================================== ======= ========\n",
        "                                           Min     Max\n",
        "    ===================================== ======= ========\n",
        "    radius (mean):                         6.981   28.11\n",
        "    texture (mean):                        9.71    39.28\n",
        "    perimeter (mean):                      43.79   188.5\n",
        "    area (mean):                           143.5   2501.0\n",
        "    smoothness (mean):                     0.053   0.163\n",
        "    compactness (mean):                    0.019   0.345\n",
        "    concavity (mean):                      0.0     0.427\n",
        "    concave points (mean):                 0.0     0.201\n",
        "    symmetry (mean):                       0.106   0.304\n",
        "    fractal dimension (mean):              0.05    0.097\n",
        "    radius (standard error):               0.112   2.873\n",
        "    texture (standard error):              0.36    4.885\n",
        "    perimeter (standard error):            0.757   21.98\n",
        "    area (standard error):                 6.802   542.2\n",
        "    smoothness (standard error):           0.002   0.031\n",
        "    compactness (standard error):          0.002   0.135\n",
        "    concavity (standard error):            0.0     0.396\n",
        "    concave points (standard error):       0.0     0.053\n",
        "    symmetry (standard error):             0.008   0.079\n",
        "    fractal dimension (standard error):    0.001   0.03\n",
        "    radius (worst):                        7.93    36.04\n",
        "    texture (worst):                       12.02   49.54\n",
        "    perimeter (worst):                     50.41   251.2\n",
        "    area (worst):                          185.2   4254.0\n",
        "    smoothness (worst):                    0.071   0.223\n",
        "    compactness (worst):                   0.027   1.058\n",
        "    concavity (worst):                     0.0     1.252\n",
        "    concave points (worst):                0.0     0.291\n",
        "    symmetry (worst):                      0.156   0.664\n",
        "    fractal dimension (worst):             0.055   0.208\n",
        "    ===================================== ======= ========\n",
        "\n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
        "\n",
        "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
        "\n",
        "    :Donor: Nick Street\n",
        "\n",
        "    :Date: November, 1995\n",
        "\n",
        "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
        "https://goo.gl/U2Uwz2\n",
        "\n",
        "Features are computed from a digitized image of a fine needle\n",
        "aspirate (FNA) of a breast mass.  They describe\n",
        "characteristics of the cell nuclei present in the image.\n",
        "A few of the images can be found at\n",
        "http://www.cs.wisc.edu/~street/images/\n",
        "\n",
        "Separating plane described above was obtained using\n",
        "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
        "Construction Via Linear Programming.\" Proceedings of the 4th\n",
        "Midwest Artificial Intelligence and Cognitive Science Society,\n",
        "pp. 97-101, 1992], a classification method which uses linear\n",
        "programming to construct a decision tree.  Relevant features\n",
        "were selected using an exhaustive search in the space of 1-4\n",
        "features and 1-3 separating planes.\n",
        "\n",
        "The actual linear program used to obtain the separating plane\n",
        "in the 3-dimensional space is that described in:\n",
        "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
        "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
        "Optimization Methods and Software 1, 1992, 23-34].\n",
        "\n",
        "This database is also available through the UW CS ftp server:\n",
        "\n",
        "ftp ftp.cs.wisc.edu\n",
        "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
        "\n",
        "References\n",
        "----------\n",
        "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
        "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
        "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870, \n",
        "     San Jose, CA, 1993. \n",
        "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
        "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
        "     July-August 1995.\n",
        "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
        "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
        "     163-171.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris = sklearn.datasets.load_iris()\n",
      "print iris.DESCR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Wisconsin\n",
      "\n",
      "In the Wisconsin breast cancer database, you are trying to predict whether\n",
      "a tumour is malignant or benign. The database consists of the measurements\n",
      "of the tumour (bc.data) and the nature of the tumour (bc.target) -- 1 = malignant, 0 == benign.\n",
      "\n",
      "Try using various combinations of parameters in a logistic regression.\n",
      "\n",
      "Validate your results with a cross cut validation\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "malignancy = pandas.Series(bc.target)\n",
      "malignancy.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "1    357\n",
        "0    212\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "column_names = ['radius', 'texture', 'perimeter', 'area', 'smoothness',\n",
      "    'compactness', 'concavity', 'concave points', 'symmetry', \n",
      "    'fractal dimension', 'radius', 'texture', 'perimeter', 'area',\n",
      "    'smoothness', 'compactness', 'concavity', 'concave points',\n",
      "    'symmetry', 'fractal dimension', 'radius', 'texture', 'perimeter',\n",
      "    'area', 'smoothness', 'compactness', 'concavity', \n",
      "    'concave points', 'symmetry', 'fractal dimension']\n",
      "data_frame_dict = {}\n",
      "for column_idx in range(len(column_names)):\n",
      "    data_frame_dict[column_names[column_idx]] = bc.data[:,column_idx]\n",
      "bc_df = pandas.DataFrame(data_frame_dict)\n",
      "bc_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>area</th>\n",
        "      <th>compactness</th>\n",
        "      <th>concave points</th>\n",
        "      <th>concavity</th>\n",
        "      <th>fractal dimension</th>\n",
        "      <th>perimeter</th>\n",
        "      <th>radius</th>\n",
        "      <th>smoothness</th>\n",
        "      <th>symmetry</th>\n",
        "      <th>texture</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>2019.0</td>\n",
        "      <td>0.66560</td>\n",
        "      <td>0.26540</td>\n",
        "      <td>0.71190</td>\n",
        "      <td>0.11890</td>\n",
        "      <td>184.60</td>\n",
        "      <td>25.380</td>\n",
        "      <td>0.16220</td>\n",
        "      <td>0.4601</td>\n",
        "      <td>17.33</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>1956.0</td>\n",
        "      <td>0.18660</td>\n",
        "      <td>0.18600</td>\n",
        "      <td>0.24160</td>\n",
        "      <td>0.08902</td>\n",
        "      <td>158.80</td>\n",
        "      <td>24.990</td>\n",
        "      <td>0.12380</td>\n",
        "      <td>0.2750</td>\n",
        "      <td>23.41</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>1709.0</td>\n",
        "      <td>0.42450</td>\n",
        "      <td>0.24300</td>\n",
        "      <td>0.45040</td>\n",
        "      <td>0.08758</td>\n",
        "      <td>152.50</td>\n",
        "      <td>23.570</td>\n",
        "      <td>0.14440</td>\n",
        "      <td>0.3613</td>\n",
        "      <td>25.53</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>567.7</td>\n",
        "      <td>0.86630</td>\n",
        "      <td>0.25750</td>\n",
        "      <td>0.68690</td>\n",
        "      <td>0.17300</td>\n",
        "      <td>98.87</td>\n",
        "      <td>14.910</td>\n",
        "      <td>0.20980</td>\n",
        "      <td>0.6638</td>\n",
        "      <td>26.50</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>1575.0</td>\n",
        "      <td>0.20500</td>\n",
        "      <td>0.16250</td>\n",
        "      <td>0.40000</td>\n",
        "      <td>0.07678</td>\n",
        "      <td>152.20</td>\n",
        "      <td>22.540</td>\n",
        "      <td>0.13740</td>\n",
        "      <td>0.2364</td>\n",
        "      <td>16.67</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>741.6</td>\n",
        "      <td>0.52490</td>\n",
        "      <td>0.17410</td>\n",
        "      <td>0.53550</td>\n",
        "      <td>0.12440</td>\n",
        "      <td>103.40</td>\n",
        "      <td>15.470</td>\n",
        "      <td>0.17910</td>\n",
        "      <td>0.3985</td>\n",
        "      <td>23.75</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>1606.0</td>\n",
        "      <td>0.25760</td>\n",
        "      <td>0.19320</td>\n",
        "      <td>0.37840</td>\n",
        "      <td>0.08368</td>\n",
        "      <td>153.20</td>\n",
        "      <td>22.880</td>\n",
        "      <td>0.14420</td>\n",
        "      <td>0.3063</td>\n",
        "      <td>27.66</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>897.0</td>\n",
        "      <td>0.36820</td>\n",
        "      <td>0.15560</td>\n",
        "      <td>0.26780</td>\n",
        "      <td>0.11510</td>\n",
        "      <td>110.60</td>\n",
        "      <td>17.060</td>\n",
        "      <td>0.16540</td>\n",
        "      <td>0.3196</td>\n",
        "      <td>28.14</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>739.3</td>\n",
        "      <td>0.54010</td>\n",
        "      <td>0.20600</td>\n",
        "      <td>0.53900</td>\n",
        "      <td>0.10720</td>\n",
        "      <td>106.20</td>\n",
        "      <td>15.490</td>\n",
        "      <td>0.17030</td>\n",
        "      <td>0.4378</td>\n",
        "      <td>30.73</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>711.4</td>\n",
        "      <td>1.05800</td>\n",
        "      <td>0.22100</td>\n",
        "      <td>1.10500</td>\n",
        "      <td>0.20750</td>\n",
        "      <td>97.65</td>\n",
        "      <td>15.090</td>\n",
        "      <td>0.18530</td>\n",
        "      <td>0.4366</td>\n",
        "      <td>40.68</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>1150.0</td>\n",
        "      <td>0.15510</td>\n",
        "      <td>0.09975</td>\n",
        "      <td>0.14590</td>\n",
        "      <td>0.08452</td>\n",
        "      <td>123.80</td>\n",
        "      <td>19.190</td>\n",
        "      <td>0.11810</td>\n",
        "      <td>0.2948</td>\n",
        "      <td>33.88</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>1299.0</td>\n",
        "      <td>0.56090</td>\n",
        "      <td>0.18100</td>\n",
        "      <td>0.39650</td>\n",
        "      <td>0.10480</td>\n",
        "      <td>136.50</td>\n",
        "      <td>20.420</td>\n",
        "      <td>0.13960</td>\n",
        "      <td>0.3792</td>\n",
        "      <td>27.28</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>1332.0</td>\n",
        "      <td>0.39030</td>\n",
        "      <td>0.17670</td>\n",
        "      <td>0.36390</td>\n",
        "      <td>0.10230</td>\n",
        "      <td>151.70</td>\n",
        "      <td>20.960</td>\n",
        "      <td>0.10370</td>\n",
        "      <td>0.3176</td>\n",
        "      <td>29.94</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>876.5</td>\n",
        "      <td>0.19240</td>\n",
        "      <td>0.11190</td>\n",
        "      <td>0.23220</td>\n",
        "      <td>0.06287</td>\n",
        "      <td>112.00</td>\n",
        "      <td>16.840</td>\n",
        "      <td>0.11310</td>\n",
        "      <td>0.2809</td>\n",
        "      <td>27.66</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>697.7</td>\n",
        "      <td>0.77250</td>\n",
        "      <td>0.22080</td>\n",
        "      <td>0.69430</td>\n",
        "      <td>0.14310</td>\n",
        "      <td>108.80</td>\n",
        "      <td>15.030</td>\n",
        "      <td>0.16510</td>\n",
        "      <td>0.3596</td>\n",
        "      <td>32.01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>943.2</td>\n",
        "      <td>0.65770</td>\n",
        "      <td>0.17120</td>\n",
        "      <td>0.70260</td>\n",
        "      <td>0.13410</td>\n",
        "      <td>124.10</td>\n",
        "      <td>17.460</td>\n",
        "      <td>0.16780</td>\n",
        "      <td>0.4218</td>\n",
        "      <td>37.13</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>1138.0</td>\n",
        "      <td>0.18710</td>\n",
        "      <td>0.16090</td>\n",
        "      <td>0.29140</td>\n",
        "      <td>0.08216</td>\n",
        "      <td>123.40</td>\n",
        "      <td>19.070</td>\n",
        "      <td>0.14640</td>\n",
        "      <td>0.3029</td>\n",
        "      <td>30.88</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>1315.0</td>\n",
        "      <td>0.42330</td>\n",
        "      <td>0.20730</td>\n",
        "      <td>0.47840</td>\n",
        "      <td>0.11420</td>\n",
        "      <td>136.80</td>\n",
        "      <td>20.960</td>\n",
        "      <td>0.17890</td>\n",
        "      <td>0.3706</td>\n",
        "      <td>31.48</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>2398.0</td>\n",
        "      <td>0.31500</td>\n",
        "      <td>0.23880</td>\n",
        "      <td>0.53720</td>\n",
        "      <td>0.07615</td>\n",
        "      <td>186.80</td>\n",
        "      <td>27.320</td>\n",
        "      <td>0.15120</td>\n",
        "      <td>0.2768</td>\n",
        "      <td>30.88</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>711.2</td>\n",
        "      <td>0.17730</td>\n",
        "      <td>0.12880</td>\n",
        "      <td>0.23900</td>\n",
        "      <td>0.07259</td>\n",
        "      <td>99.70</td>\n",
        "      <td>15.110</td>\n",
        "      <td>0.14400</td>\n",
        "      <td>0.2977</td>\n",
        "      <td>19.26</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td>630.5</td>\n",
        "      <td>0.27760</td>\n",
        "      <td>0.07283</td>\n",
        "      <td>0.18900</td>\n",
        "      <td>0.08183</td>\n",
        "      <td>96.09</td>\n",
        "      <td>14.500</td>\n",
        "      <td>0.13120</td>\n",
        "      <td>0.3184</td>\n",
        "      <td>20.49</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td>314.9</td>\n",
        "      <td>0.11480</td>\n",
        "      <td>0.06227</td>\n",
        "      <td>0.08867</td>\n",
        "      <td>0.07773</td>\n",
        "      <td>65.13</td>\n",
        "      <td>10.230</td>\n",
        "      <td>0.13240</td>\n",
        "      <td>0.2450</td>\n",
        "      <td>15.66</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td>980.9</td>\n",
        "      <td>0.59540</td>\n",
        "      <td>0.23930</td>\n",
        "      <td>0.63050</td>\n",
        "      <td>0.09946</td>\n",
        "      <td>125.10</td>\n",
        "      <td>18.070</td>\n",
        "      <td>0.13900</td>\n",
        "      <td>0.4667</td>\n",
        "      <td>19.08</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td>2615.0</td>\n",
        "      <td>0.26000</td>\n",
        "      <td>0.20090</td>\n",
        "      <td>0.31550</td>\n",
        "      <td>0.07526</td>\n",
        "      <td>188.00</td>\n",
        "      <td>29.170</td>\n",
        "      <td>0.14010</td>\n",
        "      <td>0.2822</td>\n",
        "      <td>35.59</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td>2215.0</td>\n",
        "      <td>0.35780</td>\n",
        "      <td>0.20950</td>\n",
        "      <td>0.46950</td>\n",
        "      <td>0.09564</td>\n",
        "      <td>177.00</td>\n",
        "      <td>26.460</td>\n",
        "      <td>0.18050</td>\n",
        "      <td>0.3613</td>\n",
        "      <td>31.56</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td>1461.0</td>\n",
        "      <td>0.39490</td>\n",
        "      <td>0.25500</td>\n",
        "      <td>0.38530</td>\n",
        "      <td>0.10590</td>\n",
        "      <td>152.40</td>\n",
        "      <td>22.250</td>\n",
        "      <td>0.15450</td>\n",
        "      <td>0.4066</td>\n",
        "      <td>21.40</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td>896.9</td>\n",
        "      <td>0.66430</td>\n",
        "      <td>0.27010</td>\n",
        "      <td>0.55390</td>\n",
        "      <td>0.12750</td>\n",
        "      <td>122.40</td>\n",
        "      <td>17.620</td>\n",
        "      <td>0.15250</td>\n",
        "      <td>0.4264</td>\n",
        "      <td>33.21</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td>1403.0</td>\n",
        "      <td>0.21170</td>\n",
        "      <td>0.14900</td>\n",
        "      <td>0.34460</td>\n",
        "      <td>0.07421</td>\n",
        "      <td>139.90</td>\n",
        "      <td>21.310</td>\n",
        "      <td>0.13380</td>\n",
        "      <td>0.2341</td>\n",
        "      <td>27.26</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td>1269.0</td>\n",
        "      <td>0.61100</td>\n",
        "      <td>0.20240</td>\n",
        "      <td>0.63350</td>\n",
        "      <td>0.09876</td>\n",
        "      <td>149.30</td>\n",
        "      <td>20.270</td>\n",
        "      <td>0.16410</td>\n",
        "      <td>0.4027</td>\n",
        "      <td>36.71</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td>1227.0</td>\n",
        "      <td>0.28120</td>\n",
        "      <td>0.14560</td>\n",
        "      <td>0.24890</td>\n",
        "      <td>0.07919</td>\n",
        "      <td>134.90</td>\n",
        "      <td>20.010</td>\n",
        "      <td>0.12550</td>\n",
        "      <td>0.2756</td>\n",
        "      <td>19.52</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>539</th>\n",
        "      <td>223.6</td>\n",
        "      <td>0.30640</td>\n",
        "      <td>0.05000</td>\n",
        "      <td>0.33930</td>\n",
        "      <td>0.10660</td>\n",
        "      <td>54.49</td>\n",
        "      <td>8.678</td>\n",
        "      <td>0.15960</td>\n",
        "      <td>0.2790</td>\n",
        "      <td>31.89</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>540</th>\n",
        "      <td>457.8</td>\n",
        "      <td>0.21180</td>\n",
        "      <td>0.06918</td>\n",
        "      <td>0.17970</td>\n",
        "      <td>0.08134</td>\n",
        "      <td>78.78</td>\n",
        "      <td>12.260</td>\n",
        "      <td>0.13450</td>\n",
        "      <td>0.2329</td>\n",
        "      <td>19.68</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>541</th>\n",
        "      <td>808.9</td>\n",
        "      <td>0.42020</td>\n",
        "      <td>0.12050</td>\n",
        "      <td>0.40400</td>\n",
        "      <td>0.10230</td>\n",
        "      <td>113.50</td>\n",
        "      <td>16.220</td>\n",
        "      <td>0.13400</td>\n",
        "      <td>0.3187</td>\n",
        "      <td>31.73</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>542</th>\n",
        "      <td>826.4</td>\n",
        "      <td>0.13760</td>\n",
        "      <td>0.10950</td>\n",
        "      <td>0.16110</td>\n",
        "      <td>0.06956</td>\n",
        "      <td>107.40</td>\n",
        "      <td>16.510</td>\n",
        "      <td>0.10600</td>\n",
        "      <td>0.2722</td>\n",
        "      <td>32.29</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>543</th>\n",
        "      <td>629.6</td>\n",
        "      <td>0.13810</td>\n",
        "      <td>0.07958</td>\n",
        "      <td>0.10620</td>\n",
        "      <td>0.06443</td>\n",
        "      <td>92.48</td>\n",
        "      <td>14.370</td>\n",
        "      <td>0.10720</td>\n",
        "      <td>0.2473</td>\n",
        "      <td>37.17</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>544</th>\n",
        "      <td>688.6</td>\n",
        "      <td>0.20370</td>\n",
        "      <td>0.06845</td>\n",
        "      <td>0.13770</td>\n",
        "      <td>0.08492</td>\n",
        "      <td>99.17</td>\n",
        "      <td>15.050</td>\n",
        "      <td>0.12640</td>\n",
        "      <td>0.2249</td>\n",
        "      <td>24.75</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>545</th>\n",
        "      <td>729.8</td>\n",
        "      <td>0.15170</td>\n",
        "      <td>0.07174</td>\n",
        "      <td>0.10490</td>\n",
        "      <td>0.06953</td>\n",
        "      <td>97.58</td>\n",
        "      <td>15.350</td>\n",
        "      <td>0.12160</td>\n",
        "      <td>0.2642</td>\n",
        "      <td>29.09</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>546</th>\n",
        "      <td>384.9</td>\n",
        "      <td>0.08842</td>\n",
        "      <td>0.02381</td>\n",
        "      <td>0.04384</td>\n",
        "      <td>0.07399</td>\n",
        "      <td>71.12</td>\n",
        "      <td>11.250</td>\n",
        "      <td>0.12850</td>\n",
        "      <td>0.2681</td>\n",
        "      <td>21.77</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>547</th>\n",
        "      <td>357.4</td>\n",
        "      <td>0.22460</td>\n",
        "      <td>0.08333</td>\n",
        "      <td>0.17830</td>\n",
        "      <td>0.09479</td>\n",
        "      <td>71.08</td>\n",
        "      <td>10.830</td>\n",
        "      <td>0.14610</td>\n",
        "      <td>0.2691</td>\n",
        "      <td>22.04</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>548</th>\n",
        "      <td>364.2</td>\n",
        "      <td>0.09546</td>\n",
        "      <td>0.03846</td>\n",
        "      <td>0.09350</td>\n",
        "      <td>0.07920</td>\n",
        "      <td>69.10</td>\n",
        "      <td>10.930</td>\n",
        "      <td>0.11990</td>\n",
        "      <td>0.2552</td>\n",
        "      <td>25.59</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>549</th>\n",
        "      <td>505.6</td>\n",
        "      <td>0.16330</td>\n",
        "      <td>0.03264</td>\n",
        "      <td>0.06194</td>\n",
        "      <td>0.07626</td>\n",
        "      <td>83.90</td>\n",
        "      <td>13.030</td>\n",
        "      <td>0.12040</td>\n",
        "      <td>0.3059</td>\n",
        "      <td>31.45</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>550</th>\n",
        "      <td>412.3</td>\n",
        "      <td>0.07348</td>\n",
        "      <td>0.00000</td>\n",
        "      <td>0.00000</td>\n",
        "      <td>0.06592</td>\n",
        "      <td>74.08</td>\n",
        "      <td>11.660</td>\n",
        "      <td>0.10010</td>\n",
        "      <td>0.2458</td>\n",
        "      <td>24.77</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>551</th>\n",
        "      <td>436.6</td>\n",
        "      <td>0.17820</td>\n",
        "      <td>0.06413</td>\n",
        "      <td>0.15640</td>\n",
        "      <td>0.08032</td>\n",
        "      <td>77.80</td>\n",
        "      <td>12.020</td>\n",
        "      <td>0.10870</td>\n",
        "      <td>0.3169</td>\n",
        "      <td>28.26</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>552</th>\n",
        "      <td>594.7</td>\n",
        "      <td>0.10640</td>\n",
        "      <td>0.06498</td>\n",
        "      <td>0.08653</td>\n",
        "      <td>0.06484</td>\n",
        "      <td>88.10</td>\n",
        "      <td>13.870</td>\n",
        "      <td>0.12340</td>\n",
        "      <td>0.2407</td>\n",
        "      <td>36.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>553</th>\n",
        "      <td>295.8</td>\n",
        "      <td>0.08298</td>\n",
        "      <td>0.02564</td>\n",
        "      <td>0.07993</td>\n",
        "      <td>0.07393</td>\n",
        "      <td>62.86</td>\n",
        "      <td>9.845</td>\n",
        "      <td>0.11030</td>\n",
        "      <td>0.2435</td>\n",
        "      <td>25.05</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>554</th>\n",
        "      <td>595.7</td>\n",
        "      <td>0.16200</td>\n",
        "      <td>0.06493</td>\n",
        "      <td>0.24390</td>\n",
        "      <td>0.07242</td>\n",
        "      <td>88.84</td>\n",
        "      <td>13.890</td>\n",
        "      <td>0.12270</td>\n",
        "      <td>0.2372</td>\n",
        "      <td>35.74</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>555</th>\n",
        "      <td>357.6</td>\n",
        "      <td>0.17100</td>\n",
        "      <td>0.09127</td>\n",
        "      <td>0.20000</td>\n",
        "      <td>0.08283</td>\n",
        "      <td>69.57</td>\n",
        "      <td>10.840</td>\n",
        "      <td>0.13840</td>\n",
        "      <td>0.2226</td>\n",
        "      <td>34.91</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>556</th>\n",
        "      <td>347.3</td>\n",
        "      <td>0.12000</td>\n",
        "      <td>0.02232</td>\n",
        "      <td>0.01005</td>\n",
        "      <td>0.06742</td>\n",
        "      <td>67.88</td>\n",
        "      <td>10.650</td>\n",
        "      <td>0.12650</td>\n",
        "      <td>0.2262</td>\n",
        "      <td>22.88</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>557</th>\n",
        "      <td>330.6</td>\n",
        "      <td>0.07158</td>\n",
        "      <td>0.00000</td>\n",
        "      <td>0.00000</td>\n",
        "      <td>0.06969</td>\n",
        "      <td>66.50</td>\n",
        "      <td>10.490</td>\n",
        "      <td>0.10730</td>\n",
        "      <td>0.2475</td>\n",
        "      <td>34.24</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>558</th>\n",
        "      <td>733.5</td>\n",
        "      <td>0.31710</td>\n",
        "      <td>0.11050</td>\n",
        "      <td>0.36620</td>\n",
        "      <td>0.08004</td>\n",
        "      <td>105.90</td>\n",
        "      <td>15.480</td>\n",
        "      <td>0.10260</td>\n",
        "      <td>0.2258</td>\n",
        "      <td>27.27</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>559</th>\n",
        "      <td>474.2</td>\n",
        "      <td>0.25170</td>\n",
        "      <td>0.09653</td>\n",
        "      <td>0.36300</td>\n",
        "      <td>0.08732</td>\n",
        "      <td>82.28</td>\n",
        "      <td>12.480</td>\n",
        "      <td>0.12980</td>\n",
        "      <td>0.2112</td>\n",
        "      <td>37.16</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>560</th>\n",
        "      <td>706.7</td>\n",
        "      <td>0.22640</td>\n",
        "      <td>0.10480</td>\n",
        "      <td>0.13260</td>\n",
        "      <td>0.08321</td>\n",
        "      <td>100.20</td>\n",
        "      <td>15.300</td>\n",
        "      <td>0.12410</td>\n",
        "      <td>0.2250</td>\n",
        "      <td>33.17</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>561</th>\n",
        "      <td>439.6</td>\n",
        "      <td>0.05494</td>\n",
        "      <td>0.00000</td>\n",
        "      <td>0.00000</td>\n",
        "      <td>0.05905</td>\n",
        "      <td>75.19</td>\n",
        "      <td>11.920</td>\n",
        "      <td>0.09267</td>\n",
        "      <td>0.1566</td>\n",
        "      <td>38.30</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>562</th>\n",
        "      <td>915.0</td>\n",
        "      <td>0.79170</td>\n",
        "      <td>0.23560</td>\n",
        "      <td>1.17000</td>\n",
        "      <td>0.14090</td>\n",
        "      <td>128.70</td>\n",
        "      <td>17.520</td>\n",
        "      <td>0.14170</td>\n",
        "      <td>0.4089</td>\n",
        "      <td>42.79</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>563</th>\n",
        "      <td>1819.0</td>\n",
        "      <td>0.41860</td>\n",
        "      <td>0.25420</td>\n",
        "      <td>0.65990</td>\n",
        "      <td>0.09873</td>\n",
        "      <td>179.10</td>\n",
        "      <td>24.290</td>\n",
        "      <td>0.14070</td>\n",
        "      <td>0.2929</td>\n",
        "      <td>29.41</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>564</th>\n",
        "      <td>2027.0</td>\n",
        "      <td>0.21130</td>\n",
        "      <td>0.22160</td>\n",
        "      <td>0.41070</td>\n",
        "      <td>0.07115</td>\n",
        "      <td>166.10</td>\n",
        "      <td>25.450</td>\n",
        "      <td>0.14100</td>\n",
        "      <td>0.2060</td>\n",
        "      <td>26.40</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>565</th>\n",
        "      <td>1731.0</td>\n",
        "      <td>0.19220</td>\n",
        "      <td>0.16280</td>\n",
        "      <td>0.32150</td>\n",
        "      <td>0.06637</td>\n",
        "      <td>155.00</td>\n",
        "      <td>23.690</td>\n",
        "      <td>0.11660</td>\n",
        "      <td>0.2572</td>\n",
        "      <td>38.25</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>566</th>\n",
        "      <td>1124.0</td>\n",
        "      <td>0.30940</td>\n",
        "      <td>0.14180</td>\n",
        "      <td>0.34030</td>\n",
        "      <td>0.07820</td>\n",
        "      <td>126.70</td>\n",
        "      <td>18.980</td>\n",
        "      <td>0.11390</td>\n",
        "      <td>0.2218</td>\n",
        "      <td>34.12</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>567</th>\n",
        "      <td>1821.0</td>\n",
        "      <td>0.86810</td>\n",
        "      <td>0.26500</td>\n",
        "      <td>0.93870</td>\n",
        "      <td>0.12400</td>\n",
        "      <td>184.60</td>\n",
        "      <td>25.740</td>\n",
        "      <td>0.16500</td>\n",
        "      <td>0.4087</td>\n",
        "      <td>39.42</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>568</th>\n",
        "      <td>268.6</td>\n",
        "      <td>0.06444</td>\n",
        "      <td>0.00000</td>\n",
        "      <td>0.00000</td>\n",
        "      <td>0.07039</td>\n",
        "      <td>59.16</td>\n",
        "      <td>9.456</td>\n",
        "      <td>0.08996</td>\n",
        "      <td>0.2871</td>\n",
        "      <td>30.37</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>569 rows \u00d7 10 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "       area  compactness  concave points  concavity  fractal dimension  \\\n",
        "0    2019.0      0.66560         0.26540    0.71190            0.11890   \n",
        "1    1956.0      0.18660         0.18600    0.24160            0.08902   \n",
        "2    1709.0      0.42450         0.24300    0.45040            0.08758   \n",
        "3     567.7      0.86630         0.25750    0.68690            0.17300   \n",
        "4    1575.0      0.20500         0.16250    0.40000            0.07678   \n",
        "5     741.6      0.52490         0.17410    0.53550            0.12440   \n",
        "6    1606.0      0.25760         0.19320    0.37840            0.08368   \n",
        "7     897.0      0.36820         0.15560    0.26780            0.11510   \n",
        "8     739.3      0.54010         0.20600    0.53900            0.10720   \n",
        "9     711.4      1.05800         0.22100    1.10500            0.20750   \n",
        "10   1150.0      0.15510         0.09975    0.14590            0.08452   \n",
        "11   1299.0      0.56090         0.18100    0.39650            0.10480   \n",
        "12   1332.0      0.39030         0.17670    0.36390            0.10230   \n",
        "13    876.5      0.19240         0.11190    0.23220            0.06287   \n",
        "14    697.7      0.77250         0.22080    0.69430            0.14310   \n",
        "15    943.2      0.65770         0.17120    0.70260            0.13410   \n",
        "16   1138.0      0.18710         0.16090    0.29140            0.08216   \n",
        "17   1315.0      0.42330         0.20730    0.47840            0.11420   \n",
        "18   2398.0      0.31500         0.23880    0.53720            0.07615   \n",
        "19    711.2      0.17730         0.12880    0.23900            0.07259   \n",
        "20    630.5      0.27760         0.07283    0.18900            0.08183   \n",
        "21    314.9      0.11480         0.06227    0.08867            0.07773   \n",
        "22    980.9      0.59540         0.23930    0.63050            0.09946   \n",
        "23   2615.0      0.26000         0.20090    0.31550            0.07526   \n",
        "24   2215.0      0.35780         0.20950    0.46950            0.09564   \n",
        "25   1461.0      0.39490         0.25500    0.38530            0.10590   \n",
        "26    896.9      0.66430         0.27010    0.55390            0.12750   \n",
        "27   1403.0      0.21170         0.14900    0.34460            0.07421   \n",
        "28   1269.0      0.61100         0.20240    0.63350            0.09876   \n",
        "29   1227.0      0.28120         0.14560    0.24890            0.07919   \n",
        "..      ...          ...             ...        ...                ...   \n",
        "539   223.6      0.30640         0.05000    0.33930            0.10660   \n",
        "540   457.8      0.21180         0.06918    0.17970            0.08134   \n",
        "541   808.9      0.42020         0.12050    0.40400            0.10230   \n",
        "542   826.4      0.13760         0.10950    0.16110            0.06956   \n",
        "543   629.6      0.13810         0.07958    0.10620            0.06443   \n",
        "544   688.6      0.20370         0.06845    0.13770            0.08492   \n",
        "545   729.8      0.15170         0.07174    0.10490            0.06953   \n",
        "546   384.9      0.08842         0.02381    0.04384            0.07399   \n",
        "547   357.4      0.22460         0.08333    0.17830            0.09479   \n",
        "548   364.2      0.09546         0.03846    0.09350            0.07920   \n",
        "549   505.6      0.16330         0.03264    0.06194            0.07626   \n",
        "550   412.3      0.07348         0.00000    0.00000            0.06592   \n",
        "551   436.6      0.17820         0.06413    0.15640            0.08032   \n",
        "552   594.7      0.10640         0.06498    0.08653            0.06484   \n",
        "553   295.8      0.08298         0.02564    0.07993            0.07393   \n",
        "554   595.7      0.16200         0.06493    0.24390            0.07242   \n",
        "555   357.6      0.17100         0.09127    0.20000            0.08283   \n",
        "556   347.3      0.12000         0.02232    0.01005            0.06742   \n",
        "557   330.6      0.07158         0.00000    0.00000            0.06969   \n",
        "558   733.5      0.31710         0.11050    0.36620            0.08004   \n",
        "559   474.2      0.25170         0.09653    0.36300            0.08732   \n",
        "560   706.7      0.22640         0.10480    0.13260            0.08321   \n",
        "561   439.6      0.05494         0.00000    0.00000            0.05905   \n",
        "562   915.0      0.79170         0.23560    1.17000            0.14090   \n",
        "563  1819.0      0.41860         0.25420    0.65990            0.09873   \n",
        "564  2027.0      0.21130         0.22160    0.41070            0.07115   \n",
        "565  1731.0      0.19220         0.16280    0.32150            0.06637   \n",
        "566  1124.0      0.30940         0.14180    0.34030            0.07820   \n",
        "567  1821.0      0.86810         0.26500    0.93870            0.12400   \n",
        "568   268.6      0.06444         0.00000    0.00000            0.07039   \n",
        "\n",
        "     perimeter  radius  smoothness  symmetry  texture  \n",
        "0       184.60  25.380     0.16220    0.4601    17.33  \n",
        "1       158.80  24.990     0.12380    0.2750    23.41  \n",
        "2       152.50  23.570     0.14440    0.3613    25.53  \n",
        "3        98.87  14.910     0.20980    0.6638    26.50  \n",
        "4       152.20  22.540     0.13740    0.2364    16.67  \n",
        "5       103.40  15.470     0.17910    0.3985    23.75  \n",
        "6       153.20  22.880     0.14420    0.3063    27.66  \n",
        "7       110.60  17.060     0.16540    0.3196    28.14  \n",
        "8       106.20  15.490     0.17030    0.4378    30.73  \n",
        "9        97.65  15.090     0.18530    0.4366    40.68  \n",
        "10      123.80  19.190     0.11810    0.2948    33.88  \n",
        "11      136.50  20.420     0.13960    0.3792    27.28  \n",
        "12      151.70  20.960     0.10370    0.3176    29.94  \n",
        "13      112.00  16.840     0.11310    0.2809    27.66  \n",
        "14      108.80  15.030     0.16510    0.3596    32.01  \n",
        "15      124.10  17.460     0.16780    0.4218    37.13  \n",
        "16      123.40  19.070     0.14640    0.3029    30.88  \n",
        "17      136.80  20.960     0.17890    0.3706    31.48  \n",
        "18      186.80  27.320     0.15120    0.2768    30.88  \n",
        "19       99.70  15.110     0.14400    0.2977    19.26  \n",
        "20       96.09  14.500     0.13120    0.3184    20.49  \n",
        "21       65.13  10.230     0.13240    0.2450    15.66  \n",
        "22      125.10  18.070     0.13900    0.4667    19.08  \n",
        "23      188.00  29.170     0.14010    0.2822    35.59  \n",
        "24      177.00  26.460     0.18050    0.3613    31.56  \n",
        "25      152.40  22.250     0.15450    0.4066    21.40  \n",
        "26      122.40  17.620     0.15250    0.4264    33.21  \n",
        "27      139.90  21.310     0.13380    0.2341    27.26  \n",
        "28      149.30  20.270     0.16410    0.4027    36.71  \n",
        "29      134.90  20.010     0.12550    0.2756    19.52  \n",
        "..         ...     ...         ...       ...      ...  \n",
        "539      54.49   8.678     0.15960    0.2790    31.89  \n",
        "540      78.78  12.260     0.13450    0.2329    19.68  \n",
        "541     113.50  16.220     0.13400    0.3187    31.73  \n",
        "542     107.40  16.510     0.10600    0.2722    32.29  \n",
        "543      92.48  14.370     0.10720    0.2473    37.17  \n",
        "544      99.17  15.050     0.12640    0.2249    24.75  \n",
        "545      97.58  15.350     0.12160    0.2642    29.09  \n",
        "546      71.12  11.250     0.12850    0.2681    21.77  \n",
        "547      71.08  10.830     0.14610    0.2691    22.04  \n",
        "548      69.10  10.930     0.11990    0.2552    25.59  \n",
        "549      83.90  13.030     0.12040    0.3059    31.45  \n",
        "550      74.08  11.660     0.10010    0.2458    24.77  \n",
        "551      77.80  12.020     0.10870    0.3169    28.26  \n",
        "552      88.10  13.870     0.12340    0.2407    36.00  \n",
        "553      62.86   9.845     0.11030    0.2435    25.05  \n",
        "554      88.84  13.890     0.12270    0.2372    35.74  \n",
        "555      69.57  10.840     0.13840    0.2226    34.91  \n",
        "556      67.88  10.650     0.12650    0.2262    22.88  \n",
        "557      66.50  10.490     0.10730    0.2475    34.24  \n",
        "558     105.90  15.480     0.10260    0.2258    27.27  \n",
        "559      82.28  12.480     0.12980    0.2112    37.16  \n",
        "560     100.20  15.300     0.12410    0.2250    33.17  \n",
        "561      75.19  11.920     0.09267    0.1566    38.30  \n",
        "562     128.70  17.520     0.14170    0.4089    42.79  \n",
        "563     179.10  24.290     0.14070    0.2929    29.41  \n",
        "564     166.10  25.450     0.14100    0.2060    26.40  \n",
        "565     155.00  23.690     0.11660    0.2572    38.25  \n",
        "566     126.70  18.980     0.11390    0.2218    34.12  \n",
        "567     184.60  25.740     0.16500    0.4087    39.42  \n",
        "568      59.16   9.456     0.08996    0.2871    30.37  \n",
        "\n",
        "[569 rows x 10 columns]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg = sklearn.linear_model.LogisticRegression()\n",
      "sklearn.cross_validation.cross_val_score(logreg,\n",
      "                                         bc_df,\n",
      "                                         malignancy,\n",
      "                                         cv=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([ 0.92173913,  0.93043478,  0.98230088,  0.9380531 ,  0.97345133])"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg.fit(bc_df, malignancy)\n",
      "zip(column_names, list(logreg.coef_[0])), logreg.intercept_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "([('radius', -0.029954116156879866),\n",
        "  ('texture', -2.1240224408316299),\n",
        "  ('perimeter', -0.95438548369821019),\n",
        "  ('area', -2.5329808524460216),\n",
        "  ('smoothness', -0.26111528855085003),\n",
        "  ('compactness', -0.049429783713233991),\n",
        "  ('concavity', 2.2092972187985929),\n",
        "  ('concave points', -0.46482214449169829),\n",
        "  ('symmetry', -1.0541543598760852),\n",
        "  ('fractal dimension', -0.16934978367319917)],\n",
        " array([ 0.62434398]))"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can't scale, because all the smallest size tumours won't be present\n",
      "in the database (because the patient would never have seen the doctor).\n",
      "\n",
      "But we'd like to try regularisation anyway."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg = sklearn.linear_model.LogisticRegressionCV()\n",
      "logreg.fit(bc_df, malignancy)\n",
      "logreg.C_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "array([ 1291.54966501])"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regularised_logreg = sklearn.linear_model.LogisticRegression(C=logreg.C_[0])\n",
      "sklearn.cross_validation.cross_val_score(regularised_logreg,\n",
      "                                         bc_df,\n",
      "                                         malignancy,\n",
      "                                         cv=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "array([ 0.94782609,  0.93913043,  0.98230088,  0.94690265,  0.98230088])"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.grid_search\n",
      "params = { 'C' : [0.0001, 0.001, 0.01, 0.1, 1.0, \n",
      "                  10, 500, 99,  99.9, 99.99, 100, 100.01, 100.1,\n",
      "                  101, 200, 1000, 1291.54, 10000],\n",
      "          'max_iter': [100, 200],\n",
      "          'penalty': ['l1', 'l2'],\n",
      "          'tol': [0.0001, 0.001]}\n",
      "searcher = sklearn.grid_search.GridSearchCV(\n",
      "                                sklearn.linear_model.LogisticRegression(),\n",
      "                                 params,\n",
      "                                 cv=5)\n",
      "searcher.fit(bc_df, malignancy)\n",
      "searcher.best_estimator_, searcher.best_score_, searcher.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "(LogisticRegression(C=1291.54, class_weight=None, dual=False,\n",
        "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
        "           multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
        "           solver='liblinear', tol=0.001, verbose=0, warm_start=False),\n",
        " 0.97539543057996481,\n",
        " {'C': 1291.54, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001})"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regularised_logreg = sklearn.linear_model.LogisticRegression(C=logreg.C_[0])\n",
      "sklearn.cross_validation.cross_val_score(regularised_logreg,\n",
      "                                         bc_df,\n",
      "                                         malignancy,\n",
      "                                         cv=5,\n",
      "                                         scoring='accuracy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "array([ 0.94782609,  0.93913043,  0.98230088,  0.94690265,  0.98230088])"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regularised_logreg = sklearn.linear_model.LogisticRegression(C=logreg.C_[0])\n",
      "sklearn.cross_validation.cross_val_score(regularised_logreg,\n",
      "                                         bc_df,\n",
      "                                         malignancy,\n",
      "                                         cv=5,\n",
      "                                         scoring='recall')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "array([ 0.98611111,  0.97222222,  0.98591549,  0.95774648,  0.97183099])"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regularised_logreg = sklearn.linear_model.LogisticRegression(C=logreg.C_[0])\n",
      "sklearn.cross_validation.cross_val_score(regularised_logreg,\n",
      "                                         bc_df,\n",
      "                                         malignancy,\n",
      "                                         cv=5,\n",
      "                                         scoring='precision')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "array([ 0.93421053,  0.93333333,  0.98591549,  0.95774648,  1.        ])"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regularised_logreg = sklearn.linear_model.LogisticRegression(C=logreg.C_[0])\n",
      "sklearn.cross_validation.cross_val_score(regularised_logreg,\n",
      "                                         bc_df,\n",
      "                                         malignancy,\n",
      "                                         cv=5,\n",
      "                                         scoring='f1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "array([ 0.95945946,  0.95238095,  0.98591549,  0.95774648,  0.98571429])"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Irises\n",
      "\n",
      "There are three kinds of flowers in the dataset:\n",
      "\n",
      "- [Setosa](https://en.wikipedia.org/wiki/Iris_setosa) ( = 0)\n",
      "\n",
      "- [Versicolor](https://en.wikipedia.org/wiki/Iris_versicolor) ( = 1)\n",
      "\n",
      "- [Virginica](https://en.wikipedia.org/wiki/Iris_virginica) ( = 2)\n",
      "\n",
      "Try using various combinations of parameters in a logistic regression.\n",
      "\n",
      "Validate your results with a cross cut validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}